---
layout: post
title:  "zemel-et-al_2016_the_variational_fair_autoencoder"
date: 2018-02-02
comments: true
categories: NN_ps
---
#### **Reference paper**:
[Louizos, C., Swersky, K., Li, Y., Welling, M., & Zemel, R. (2015). The variational fair autoencoder. arXiv preprint arXiv:1511.00830.](https://arxiv.org/pdf/1511.00830.pdf)

#### **_Why_:**
In this paper, the authors set out to find latent representations of a dataset $$\textbf{X}$$
that are invariant to nuisance (or sensitive) variables, denoted $$\textbf{s}$$.

Obviously, for the case of nuisance variables (i.e., variables that are uninformative),
the ultimate goal is to find a a latent represenation, which removes the uninformative
parts while still retaining the maximum amount of information about the data
as possible -- think here about a subsequent classification task, where we
would want to predict $$\textbf{y}$$ from the learned latent representations
$$\textbf{z}$$.

On the other hand, when $$\textbf{s}$$ is a sensitive variable, then the goal
becomes learning a latent representation that is devoid of these variables (e.g., race,
age, gender, etc.), and thus enabling a "fair representation" when predicting
$$\textbf{y}$$. Again, thinking of the subsequent classifier, $$\tilde{p}(\cdot)$$,
we would want get rid of any learned baises, which is fully achieved if
\begin{align}
\tilde{p}(y | x,s) = \tilde{p}(y | x,\bar{s}).
\end{align}
(The hope is that by training a machine learning algorithm on $$\textbf{z}$$, that
the classifier is much closer to the bias-free condition as opposed to a classifier
trained on the original dataset.)

It is important to emphasize that in both cases, the variables $$\textbf{s}$$ must
be known.

#### **_How_:**
As its name suggests, the Variational Fair Autoencoder (VFAE) is a variant of the
[Variational Autoencoder](https://arxiv.org/pdf/1312.6114.pdf) (VAE). Similar to
the VAE, the VFAE also maximizes an expected lower bound of the log-likelihood of
$$\textbf{X}$$, which is differs from the VAE case because now we consider
the condition on the sensistive variables $$\textbf{s}$$. The VFAE maximizes
the lower bound given below:
\begin{align}
\sum_{n=1}^N \log p(x_n|s_n)\geq \mathcal{F}(\phi,\theta; x_n,s_n)
\end{align}
where $$\phi$$ and $$\theta$$ are the parameters of $$q(\cdot)$$ and $$p(\cdot)$$,
respectively -- i.e., the variational and true posteriors.

The authors consider two cases, namely: i. supervised and ii. semi-supervised. In
the first case, there is no subsequent prediction task and the expected lower
bound amounts to:
\begin{align}
\mathcal{F}(\phi,\theta; x_n,s_n) = \sum_{n=1}^N E_{q(z_n|x_n,s_n)}[\log p(x_n|z_n,s_n)] - KL(q(z_n|x_n,s_n)||p(z)),
\end{align}
where $$KL$$ denotes the Kullback-Liebler divergence between two probability
distributions over a common population.

In the semi-supervised case, the latent representation $$z_1$$ is further
decomposed into two independent components: another latent represenation $$z_2$$
and the label $$y$$. The idea being here that while our goal is to purge $$s$$
from our represenation, we still like to retain any predictive power $$s$$ has
for predicting $$y$$. Any leakage of the sensitive variable $$s$$ is taken care
of by regularization through the Mazimum Mean Discrepancy (MMD). The resulting
expected lower bound is as follows:
$$
\begin{align}
\mathcal{F}(\phi,\theta; x_n,s_n,y_n) = \sum_{n=1}^N E_{q(z_{1n},z_{2n},y_n|x_n,s_n)}
[\log p(z_{2n})] &+ \log p(y_n) + \log p(z_{1n}|z_{2n},y_n) \\
&+\log p(x_n|z_{1n},s_n)- \log q(z_{1n},z_{2n}, y_n | x_n,s_n)].
\end{align}
$$

The above equation can be expressed in terms of expectations of various KL
divergence computations if certain normal distributions are used for the
priors and posteriors (see original paper for details).It also bears mentioning
that in the case where not all of the data is labelled, $$y$$ can be imputed by
the variational posterior of $$y$$ given $$z_1$$.

Training this model is done via stochastic gradient descent (which is enabled by
the "re-parametrization" trick introduced in the original VAE paper) to maximize
the expected lower bound. For estimating the expectations of the KL divergences,
monte carlo methods are used -- where a sample of one is suitable if the mini
batch size is large enough e.g., 100.

#### **_What_:**
The authors showed that by using a neural network architecture, similar to the
VAE, that the learned representations are representative of the original dataset;
and that when applied to a subsequent classification task can significantly
improve the discrimination factor (i.e., a measure of fairness) when compared to
a model trained on the original set of features.
