---
layout: post
title:  "amjadian-et-al_2017_term_extraction"
date: 2017-10-29
comments: true
categories: NN_ps
---
#### **Reference paper**:

E. Amjadian, D. Inkpen, T.S. Paribakht, and F. Faez (2017). Local-global vectors
to improve unigram terminology extraction.

#### **_Why_:**

In order to begin to understand a specific domain, one must know the commonly used
terminology. Moreover, terms of a specific domain often represent important notions
or ideas, and so, knowledge of these would enable effective communication and learning.
For example, in my quest to [learn functional programming]({{ site.baseurl }}{% link fpscala.md %}),
knowing that a *side-effect* is an important term would motivate me to learn what
this means as well as understand its significance to this programming paradigm.
For this reason, automatic term extraction (ATE) is an important task in natural
language processing (NLP).

Traditional techniques for ATE such as linguistic methods, statistical methods
and the hybrid of the two, often regard single words of a document as an atomic
element. Determining whether or not this element is indeed a term of the domain is usually
based off a single real-valued score that is associated with this word
(e.g., relative frequency). In contrast, distributed methods characterize words by a vector
of real-valued components (e.g., [word embeddings](https://en.wikipedia.org/wiki/Word_embedding));
however, to the best of the authors' knowledge, their work represents the first
successful application of distributed methods to the terminology extraction task.

The main raison d'etre of this paper is to improve upon previously existing
methods for unigram term extraction through distributed representations and neural networks.


#### **_How_:**

The authors' method for improving the performance of a previously existing term extraction
technique is to augment it with an additional *filtering* task, which uses additional information
in order to make the final determination on the identified candidate terms.
In particular, their ATE method requires a domain-specific
corpora (i.e., a corpora of interest) as well as global corpora (i.e., a reference corpora)
and consists of the following steps:

1. Use a previously developed ATE method to identify candidate terms.
2. For each candidate term, train a neural network (i.e., [word2vec](https://en.wikipedia.org/wiki/Word2vec)) to generate
a distributed representation of the word based on the domain-specific corpora. The
authors refer to this representation as **local embeddings**.
3. For each candidate term, use the general corpora to derive another distributed
representation -- called a **global embedding**. (For their application, the authors
used pre-built GloVe vectors that were trained on Wikipedia + the Gigaword 5 corpus.)
4. Apply a classifier to each candidate term, which takes both the local and
global vectors as features, and makes the final determination of whether or not
the candidate term is indeed a term.  

It should be noted that their method can be combined with any non-polysynthetic language
and any domain (provided a domain-specific and general corpora are available).

#### **_What_:**

The new ATE method was applied to extracting key terms from five high school
mathematics textbooks. Below are the specifications of the new method that was
used for this application:

- **Initial ATE method(s) for identifying candidate terms:** Termostat, Topla, and
AntConc -- the results of these three methods were combined to generate the
unique pool of candidate terms.
- **Global-embeddings method:** GloVe
- **Local-embeddings method:** word2vec (CBOW vs. skip-gram architecture)
- **Classifier:** logistic regression vs. SVM vs. MLP

In order to judge the new ATE method, two human annotators assessed the resulting
candidate terms from each of the three individual initial ATE methods. The outcome
was a dataset consisting of 954 instances with 325 positive and 629 negative cases,
which served as the ground truth labels for the classifier.

The new method was shown to have markedly better performance when compared to the
three previously developed ATE methods.

#### **_After reading this paper, I want to learn about:_**

1. [word2vec](https://en.wikipedia.org/wiki/Word2vec) and the CBOW and skip-gram
architectures and their success stories.
