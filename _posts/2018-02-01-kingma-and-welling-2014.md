---
layout: post
title:  "kingma-welling_2014_auto-encoding_variational_bayes"
date: 2018-02-01
comments: true
categories: NN_ps
---
#### **Reference paper**:
[Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.](https://arxiv.org/pdf/1312.6114.pdf)

#### **_Why_:**
The main question the authors seek to answer is one, namely: how can efficient
approximate inference be performed when learning with directed probabilistic
models whose continuous latent variables have **intractable posteriors**?

In particular, the authors consider the following problem setup:
- dataset $$\textbf{X} =\{x_i\}_{i=1}^N$$
- each datum $$x_i$$ can be thought of as being generated by a two-stage
random mechanism, where in the first stage an associated latent variable $$z_i$$ is drawn
from some prior $$p_\theta(z)$$; and in the next stage, $$x_i$$ is drawn from the conditional
distribution $$p_\theta(x|z)$$.
- both the model parameters $$\theta$$ and the latent variables $$z$$ are unknown.

The ultimate goal is to obtain ML or MAP estimates of $$\theta$$ in an efficient
manner. It is important to note that in situations where the posterior
$$p_\theta(z|x)$$ is intractable, building the expected conditional log-likelihood
(i.e., $$E_{p(z|x;\theta)}(L_c(\theta;\textbf{X}) | \textbf{X};\theta)$$) is not
possible; and so, the E-step of the EM algorithm cannot be performed.

#### **_How_:**
The authors consider a variational Bayes (VB) approach, where one optimizes
the loss function with an approximation to the intractable posterior. Specifically,
let $$q_\phi(z|x)$$ denote the variational approximation to the intractable
$$p_\theta(z|x)$$. With these definitions in place, an expected lower bound of
the marginal log-likelihood of $$\textbf{X}$$ can be obtained:

$$
\begin{align}
\log p_\theta(x_i) &=\log \left(E_{q_\phi(z|x_i)} \left[\frac{p_\theta(x_i,z)}{q_\phi(z|x_i)}\right]\right)\\
&\geq E_{q_\phi(z|x_i)} \left[\log\left(\frac{p_\theta(x_i,z)}{q_\phi(z|x_i)}\right) \right] \qquad \text{[Jensen's inequality]}\\
&= E_{q_\phi(z|x)}[\log p_\theta (x_i | z)] - KL(q_\phi(z|x_i)||p_\theta(z)) \qquad [\text{since } p(x,z)=p(x|z)p(z)]\\
&= \mathcal{L}(\theta,\phi;x_i),
\end{align}
$$

where $$KL()$$ represents the KL divergence of two probability distributions
over a common population. Note that $$\log p_\theta(x_i)$$ can also be expressed
as the sum of the lower bound derived above and the KL-divergence between $$q_\phi(z|x)$$
and $$p_\theta(z|x)$$.

Some important remarks:
- it is often the case that KL divergence term in $$\mathcal{L}(\theta,\phi;x_i)$$
is analytically tractible (i.e., determined by the assumptions on the
parametric forms of the two probability distributions), in which case we would
only need to sample from $$q_\phi(z|x)$$ to obtain estimates of the first
expectation term. In particular:
\begin{align}
E_{q_\phi(z|x)}[\log p_\theta (x_i | z)] \approx \frac{1}{L}\sum_{l =1 }^L \log (p_\theta(x_i | z_{i,l})),
\end{align}
where $$z_{i,l}\sim q_\phi (z|x_i)$$ (i.e., the $$l$$-th sample) and $$L$$ is
the number of Monte Carlo samples.
- the authors refer to the $$q_\phi(z|x)$$ as the recognition model, which can
also be interpreted as the "encoder" in autoencoder terms. Similarly, $$p_\theta(x|z)$$
can be thought of as "decoder".

So, instead of maximizing $$p_\theta(x)$$, we set out to maximize $$\mathcal{L}(\theta,\phi;x_i)$$
over $$\phi$$ and $$\theta$$, for which the authors propose maximizing these parameters
jointly via stochastic gradient descent. However, in order to utilize backpropogation,
we need a way to sample from $$q_\phi(z|x)$$ that is differentiable with respect
to $$\phi$$. To do this, the authors introduced the **re-parametrization trick**:
\begin{align}
z = g_\phi(\epsilon, x) \quad \text{and} \quad \epsilon \sim p(\epsilon)
\end{align}
where $$\epsilon$$ is some auxiliary noise and $$g_\phi(\cdot)$$ is differentiable
function with respect to $$\phi$$. (The authors note that when using mini-batch
SGD with a relatively large batch size, then $$L=1$$ is sufficient enough to
get a good estimate of the first expectation term in $$\mathcal{L}(\theta,\phi;x_i)$$.)

#### **_What_:**
The authors introduced a novel estimator of $$\mathcal{L}(\theta,\phi;x_i)$$ is called the
Stochastic Gradient Variational Bayes (SGVB) estimator, while the overall algorithm
was termed as the Auto-Encoding Variational Bayes (AEVB) algorithm. The facilitation
of the backpropogation is enabled by the newly introduce re-parametrization technique.
Their experiments showed that the AEVB produced larger values of marginal log-likelihood
when compared to related techniques such as wake-sleep, and Monte Carlo EM algorithms.

#### **_Useful Resources_:**
[Tutorial on Variational Autoencoders by Carl Doersch](https://arxiv.org/pdf/1606.05908.pdf)
